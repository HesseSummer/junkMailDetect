{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意，需要预先创建以下目录和文件\n",
    "`../glove/glove.6B.100d.txt`：下载地址：<http://nlp.stanford.edu/data/glove.6B.zip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import\n",
    "0. 读取数据、矩阵计算、路径计算等基础库\n",
    "--------\n",
    "1. 正则去标点等+大小写\n",
    "2. 空格分词+去停用词\n",
    "3. 词形统一\n",
    "3. tf-idf编码方式\n",
    "--------\n",
    "4. 模型：使用keras自己构建CNN\n",
    "5. 分析：混淆矩阵、准确率预测、训练耗时\n",
    "6. 保存模型\n",
    "## 改进点\n",
    "- GPU加速\n",
    "- 其他模型\n",
    "- 参数\n",
    "- 结果显示\n",
    "- 函数封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据集\n",
    "使用csv读取文本文件，得到二维列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n",
      "['ham', 'Ok lar... Joking wif u oni...']\n",
      "['spam', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"获取原始数据\"\"\"\n",
    "file_path = '../smsspamcollection/SMSSpamCollection'\n",
    "smsFile = open(file_path, 'r', encoding='utf-8') ## 返回文件对象\n",
    "sms = csv.reader(smsFile, delimiter='\\t') ## 第一层：行列表；第二层：列列表\n",
    "sms = list(sms) \n",
    "smsFile.close()\n",
    "\"\"\"显示原始数据\"\"\"\n",
    "for line in sms[0:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理\n",
    "## 定义预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regUse(text):\n",
    "    text = re.sub(r\"[,.?!\\\":]\", '', text) # 去标点\n",
    "    text = re.sub(r\"'\\w*\\s\", ' ', text) # 去缩写\n",
    "    text = re.sub(r\"#?&.{1,3};\", '', text) # 去html符号\n",
    "    return text.lower()\n",
    "def sampleSeg(text):\n",
    "    tokens = [word for word in word_tokenize(text) if word not in stopwords.words('english') and len(word)>=3]\n",
    "    return tokens\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "def lemSeg(tokens):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, pos in pos_tag(tokens):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "\n",
    "    return res\n",
    "def preprocess(text):\n",
    "    text = regUse(text)\n",
    "    tokens = sampleSeg(text)\n",
    "    tokens = lemSeg(tokens)\n",
    "    return tokens ## 返回的是单词列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实施预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理耗时:38.72s\n",
      "预处理后的结果示例：\n",
      "['jurong point crazy available bugis great world buffet cine get amore wat', 'lar joking wif oni', 'free entry wkly comp win cup final tkts 21st may 2005 text 87121 receive entry question std txt rate apply 08452810075over18']\n",
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "sms_data = [] ## 每个元素是一个句子\n",
    "sms_label = [] ## 每个元素是一个字符串0/1\n",
    "label_num = {\"spam\":1, \"ham\":0} ## 垃圾邮件为1，正常邮件为0\n",
    "\n",
    "start = time.perf_counter()\n",
    "for line in sms:\n",
    "    sms_data.append(\" \".join(preprocess(line[1])))\n",
    "    sms_label.append(label_num[line[0]])\n",
    "elapsed = (time.perf_counter() - start)\n",
    "print(\"预处理耗时:{:.2f}s\".format(elapsed))\n",
    "\"\"\"显示预处理结果\"\"\"\n",
    "print(\"预处理后的结果示例：\")\n",
    "print(sms_data[0:3])\n",
    "print(sms_label[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备训练集、验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小：3900\n",
      "验证集大小：1672\n"
     ]
    }
   ],
   "source": [
    "MAX_NUM_WORDS = 2000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) ## 最终选取频率前MAX_NUM_WORDS个单词\n",
    "tokenizer.fit_on_texts(sms_data)\n",
    "MAX_SEQUENCE_LENGTH = 50 ## 长度超过MAX_SEQUENCE_LENGTH则截断，不足则补0\n",
    "sequences = tokenizer.texts_to_sequences(sms_data) ## 是一个二维数值数组，每一个数值都是对应句子对应单词的**索引**\n",
    "dataset = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) \n",
    "labels = to_categorical(np.asarray(sms_label)) ## 将label转为独热编码形式\n",
    "\n",
    "\"\"\"打乱\"\"\"\n",
    "indices = np.arange(dataset.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "dataset = dataset[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "\"\"\"划分\"\"\"\n",
    "size_dataset = len(dataset)\n",
    "size_trainset = int(round(size_dataset*0.7))\n",
    "print(\"训练集大小：{}\".format(size_trainset))\n",
    "print(\"验证集大小：{}\".format(size_dataset - size_trainset))\n",
    "x_train = dataset[0:size_trainset]\n",
    "y_train = labels[0:size_trainset]\n",
    "\n",
    "x_val = dataset[size_trainset+1: size_dataset]\n",
    "y_val = labels[size_trainset+1: size_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型\n",
    "## 构建embedding层\n",
    "### 1.准备预训练好的词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备词典耗时:19.12s\n"
     ]
    }
   ],
   "source": [
    "embedding_dic = {}\n",
    "file_path = '../glove/glove.6B.100d.txt'\n",
    "\n",
    "start = time.perf_counter()\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_dic[word] = coefs\n",
    "        \n",
    "elapsed = (time.perf_counter() - start)\n",
    "print(\"准备词典耗时:{:.2f}s\".format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.准备embedding_matrix\n",
    "embedding_matrix是本文中所有（或者选取高频部分）单词对应的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有8180个单词，示例：\n",
      "['get', 'call', 'come', 'free', 'know'] [1, 2, 3, 4, 5]\n",
      "准备embedding_matrix耗时:0.01s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"选取部分单词\"\"\"\n",
    "word_index = tokenizer.word_index ## 得到一个字典，key是选择的单词，value是它的索引\n",
    "\n",
    "print(\"共有{}个单词，示例：\".format(len(word_index)))\n",
    "print(list(word_index.keys())[0:5], list(word_index.values())[0:5]) \n",
    "\n",
    "\"\"\"准备这些单词的embedding_matrix\"\"\"\n",
    "EMBEDDING_DIM = 100 ## 令词向量的维度是100\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1) ## 为什么要加一？\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "start = time.perf_counter()\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embedding_dic.get(word)\n",
    "    if embedding_vector is not None: ## 单词在emmbeding_dic中存在时\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "elapsed = (time.perf_counter() - start)\n",
    "print(\"准备embedding_matrix耗时:{:.2f}s\".format(elapsed))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.构建embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0707 21:03:50.965302 18976 deprecation_wrapper.py:119] From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(input_dim=num_words,  # 词汇表单词数量\n",
    "                            output_dim=EMBEDDING_DIM,  # 词向量维度\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)  # 词向量矩阵不进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建、连接其他层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 100)           200000    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 46, 128)           64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 23, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 19, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 9, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 444,994\n",
      "Trainable params: 244,994\n",
      "Non-trainable params: 200,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  # 占位。\n",
    "embedded_sequences = embedding_layer(sequence_input)  # 返回 句子个数*50*100\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(label_num), activation='softmax')(x)\n",
    " \n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0707 21:12:24.232672 18976 deprecation.py:323] From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3900 samples, validate on 1671 samples\n",
      "Epoch 1/5\n",
      "3900/3900 [==============================] - 3s 827us/step - loss: 0.1905 - acc: 0.9395 - val_loss: 0.2461 - val_acc: 0.9366\n",
      "Epoch 2/5\n",
      "3900/3900 [==============================] - 3s 725us/step - loss: 0.1249 - acc: 0.9674 - val_loss: 0.1727 - val_acc: 0.9515\n",
      "Epoch 3/5\n",
      "3900/3900 [==============================] - 3s 733us/step - loss: 0.0961 - acc: 0.9790 - val_loss: 0.1931 - val_acc: 0.9485\n",
      "Epoch 4/5\n",
      "3900/3900 [==============================] - 3s 733us/step - loss: 0.0797 - acc: 0.9838 - val_loss: 0.4180 - val_acc: 0.9240\n",
      "Epoch 5/5\n",
      "3900/3900 [==============================] - 3s 704us/step - loss: 0.0878 - acc: 0.9844 - val_loss: 0.2556 - val_acc: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x115b7c2b940>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=16, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 还没有跑测试集、没有显示精度召回率等等信息"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
